---
title: 以梯度下降法解線性迴歸係數
author: Yen-Ting, Su
date: '2018-09-17'
slug: linear-gradient-descent-method
categories:
  - Programming
tags: 
  - R
  - Machine Learning
description: "此篇文章說明如何用梯度下降(Gradient Descent)方法找出線性迴歸最佳參數解。"
---



<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<hr />
<div id="程式說明" class="section level2">
<h2>程式說明</h2>
<p>此篇文章說明如何用梯度下降(Gradient Descent)方法找出線性迴歸最佳參數解。實作參考自國立台灣大學資訊網路與多媒體研究所<strong>李宏毅</strong>老師的Youtube教學影片：</p>
<ol style="list-style-type: decimal">
<li><p><a href="https://www.youtube.com/watch?v=fegAeph9UaA&amp;index=3&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49">ML Lecture 1: Regression - Case Study</a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=1UqCjFQiiy0&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=4">ML Lecture 1: Regression - Demo</a></p></li>
</ol>
<hr />
</div>
<div id="使用套件" class="section level2">
<h2>使用套件</h2>
<p><code>tidyverse</code>為R軟體專為資料科學的集合套件，裡面包含常用的<code>dplyr</code>及<code>ggplot2</code>等常用的資料科學套件。詳情資訊可至<a href="https://www.tidyverse.org/">tidyverse套件官網</a>查看。</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<hr />
</div>
<div id="使用資料集及分析問題" class="section level2">
<h2>使用資料集及分析問題</h2>
<p>本次資料集使用<code>ggplot2</code>套件內提供的鑽石(<code>diamonds</code>)資料集，裡面共包含53,940筆及10個特徵資料。分析的問題為給定在切割(cut)為理想型(Ideal)及淨度(clarity)為內部完美無瑕(IF)的鑽石資料集下，以克拉做為特徵資料(即解釋變數)，建立線性迴歸模型，預測鑽石價格(即被解釋變數)。有關鑽石的知識，可參考此<a href="https://www.alren.com.hk/CHT/CMS/Diamond-Education/14">網站</a>。</p>
<p>預測的迴歸式如下，下標<code>i</code>代表樣本：</p>
<p><span class="math display">\[Price_i=\hat{\alpha} + \hat{\beta_1} Carat_i + \varepsilon_i\]</span></p>
<p>資料整理程式碼如下所示：</p>
<pre class="r"><code>data &lt;- diamonds %&gt;% 
  filter((cut==&quot;Ideal&quot;) &amp; (clarity==&quot;IF&quot;)) %&gt;%
  select(price, carat)
data</code></pre>
<pre><code>## # A tibble: 1,212 x 2
##    price carat
##    &lt;int&gt; &lt;dbl&gt;
##  1  2783  0.52
##  2  2789  0.55
##  3  2790  0.64
##  4  2800  0.61
##  5  2802  0.53
##  6  2868  0.62
##  7  2869  0.62
##  8  2925  0.71
##  9  2952  0.74
## 10  2960  0.64
## # ... with 1,202 more rows</code></pre>
<p>經過<code>filter</code>挑選切割(cut)為理想型(Ideal)及淨度(clarity)為內部完美無瑕(IF)的鑽石資料集後，只剩下1,212筆資料。</p>
<hr />
</div>
<div id="迴歸函數與封閉解法" class="section level2">
<h2>迴歸函數與封閉解法</h2>
<p>我們直接透過R軟體的<code>lm</code>函數，直接跑線性迴歸模型，估計出參數值。</p>
<pre class="r"><code>model &lt;- lm(price ~ carat, data = data)
model</code></pre>
<pre><code>## 
## Call:
## lm(formula = price ~ carat, data = data)
## 
## Coefficients:
## (Intercept)        carat  
##       -3043        11682</code></pre>
<p>估計出來的模型的<span class="math inline">\(\hat{\alpha}=-3403\)</span>，<span class="math inline">\(\hat{\beta_1}=11682\)</span>。</p>
<p>繪製圖形觀看迴歸線配適狀況:</p>
<pre class="r"><code>ggplot(data, aes(x = carat, y = price)) + 
  geom_point() +
  stat_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-09-19-linear-gradient-descent-method_files/figure-html/diamonds%20reg%20graph-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>除透過程式的函數外，也可直接以線性迴歸的封閉解(Closed-form solution)進行計算：</p>
<p><span class="math display">\[\beta=(X^TX)^{-1}X^TY\]</span></p>
<pre class="r"><code># 整理解釋變數資料並轉為matrix格式
x &lt;- data %&gt;% 
  transmute(Intercept = 1, carat) %&gt;%
  as.matrix()

# 整理被解釋變數資料並轉為vector格式
y &lt;- data %&gt;% pull(price)

# 封閉解
beta &lt;- (solve(t(x) %*% x) %*% t(x) %*% y)

beta</code></pre>
<pre><code>##               [,1]
## Intercept -3042.77
## carat     11681.76</code></pre>
<p>可以發現封閉解解法和套件執行的結果是一樣的。</p>
<hr />
</div>
<div id="梯度下降法" class="section level2">
<h2>梯度下降法</h2>
<p>為了有「機器學習」的感覺，此處示範以梯度下降法的方式，來找出線性迴歸最佳的參數。</p>
<p>在線性迴歸模型中，誤差函數(Error Function)為均方誤差(MSE, Mean squared error)：</p>
<p><span class="math display">\[ L(\hat{\alpha}, \hat{\beta_1}) = \frac{1}{N} \sum_{n=1}^{N} (y_{i}-\hat{y_{i})}^{2}=\frac{1}{N} \sum_{n=1}^{N}(y_{i} - \hat{\alpha} - \hat{\beta_1} Carat_i)^{2}\]</span></p>
<p>接下來將誤差函數對<span class="math inline">\(\hat{\alpha}\)</span>及<span class="math inline">\(\hat{\beta_1}\)</span>進行偏微分，得到梯度下降的計算式：</p>
<p><span class="math display">\[\frac{\partial L}{\partial \hat{\alpha}} = \frac{1}{N} \sum_{n=1}^{N} 2(y_{i} - \hat{\alpha} - \hat{\beta_1} Carat_i)(-1)\]</span></p>
<p><span class="math display">\[\frac{\partial L}{\partial \hat{\beta_1}} = \frac{1}{N} \sum_{n=1}^{N} 2(y_{i} - \hat{\alpha} - \hat{\beta_1} Carat_i)(-Carat_i)\]</span></p>
<p>每次疊代時，參數依據學習比率乘上梯度值進行調整。</p>
<pre class="r"><code># 設定梯度下降參數
lr &lt;- 0.1                 # 學習比率
maxIterations &lt;- 20000    # 最大疊代次數
y &lt;- data$price           # 被解釋變數資料
x &lt;- data$carat           # 解釋變數資料
alpha &lt;- 0                # 截距項初始值
beta &lt;- 0                 # 解釋變數係數初始值
iterations &lt;- 1           # 疊代次數起始值
traningRecord &lt;- tibble(alpha, beta)   # 儲存訓練過程資訊

while(iterations &lt;= maxIterations){
  
  # 以目前的截距項及特徵係數值進行預測
  yPred &lt;- alpha + beta * x
  
  # 損失函數對截距項進行偏微分
  alphaGrad &lt;- sum(2 * (y - yPred) * (-1)) / nrow(data)  
  
  # 損失函數對特徵係數進行偏微分
  betaGrad &lt;- sum(2 * (y - yPred) * (-x)) / nrow(data)   
  
  # 提前終止條件
  if((abs(alphaGrad)&lt;=(10^(-5))) &amp; all(abs(betaGrad)&lt;=(10^(-5)))){
    
    cat(&quot;在第&quot; ,iterations,&quot;次 疊代已達收斂條件\n&quot;)
    break
    
  }else if((abs(alphaGrad)&gt;=(10^(10))) &amp; all(abs(betaGrad)&gt;=(10^(10)))){
    
    cat(&quot;在第&quot; ,iterations,&quot;次 確認數值發散，終止運作\n&quot;)
    break
  }
  
  # 調整截距項及特徵係數值
  alpha &lt;- alpha - lr * alphaGrad
  beta &lt;- beta - lr * betaGrad
  
  # 記錄疊代次數
  iterations &lt;- iterations + 1    
  
  # 記錄訓練資訊
  traningRecord &lt;- traningRecord %&gt;% 
    bind_rows(tibble(alpha, beta))
}</code></pre>
<pre><code>## 在第 1582 次 疊代已達收斂條件</code></pre>
<pre class="r"><code># 整理學習結果
tibble(Intercept = alpha, carat = beta)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   Intercept  carat
##       &lt;dbl&gt;  &lt;dbl&gt;
## 1    -3043. 11682.</code></pre>
<p>由結果可以看出，經過1,582次的訓練後，梯度下降法得到的係數值與<code>lm</code>函數及封閉解的結果相同，訓練成功！</p>
<p>我們可以透過繪圖來了解機器學習的過程，首先需先繪製出誤差函數的等高線圖，再來將學習路徑標示在圖上。</p>
<pre class="r"><code># 計算不同截距項及係數項組合下的誤差函數值
plotData &lt;- NULL
for(alpha in seq(-6000, 3000, 500)){
  for(beta in seq(0, 24000, 500)){
    
    # 計算均方誤差
    yPred &lt;- alpha+ beta %*% t(x)
    mse &lt;- sum((y-yPred)^2) / length(y)
    
    # 紀錄資訊
    plotData &lt;- plotData %&gt;% bind_rows(tibble(alpha, beta, mse))
  }
}

# 繪製誤差函數的等高線圖形 並加入學習過程
ggplot(plotData, aes(x = beta, y = alpha, z = mse)) +
  # 繪製等高線圖
  stat_contour(aes(colour = ..level..), size = 1.3, bins = 20) +
  # 修改等高線顏色
  scale_colour_gradientn(colours = rainbow(20), name = &quot;均方誤差&quot;) +
  # 修改x軸名稱
  xlab(&quot;beta&quot;) +
  # 修改y軸名稱
  ylab(&quot;alpha&quot;) +
  # 標註梯度疊代每次的點位
  geom_point(data = traningRecord, 
             mapping = aes(x = beta, y = alpha), 
             size = 1.3, shape = 21, fill = &quot;white&quot;)</code></pre>
<p><img src="/post/2018-09-19-linear-gradient-descent-method_files/figure-html/learning%20graph-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>由圖可以看出，電腦從初始值(0,0)的地方，透過梯度下降法，一路收斂到均方誤差最小的地方。</p>
<p>學習比率是梯度下降法中重要的參數，若設定太小，收斂速度會太慢。但若設定值太大，則會有梯度爆炸(Gradient explosion)的狀況出現，係數值會發散而無法收斂。</p>
<p><img src="1.png" /></p>
<p>學習比率的設定依各個資料集的狀況而有所不同，在此範例中，若學習比率設定為0.9，就會發生梯度爆炸的狀況，如下圖：</p>
<p><img src="/post/2018-09-19-linear-gradient-descent-method_files/figure-html/gradient%20explosion-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
